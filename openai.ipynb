{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "import os\n",
    "\n",
    "def get_key():\n",
    "    return (pl.Path(os.getcwd())/\".oai.key\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = get_key()\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"Convert this text to a programmatic command:\\n\\nExample: Ask Constance if we need some bread\\nOutput: send-msg `find constance` Do we need some bread?\\n\\nReach out to the ski store and figure out if I can get my skis fixed before I leave on Thursday{}\",\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.2,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"{}\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6stjH0lVS7kTTKpyNDtGbAOs8Jpjt at 0x1106be390> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nOutput: send-msg `find ski store` Can I get my skis fixed before I leave on Thursday?\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1678541955,\n",
       "  \"id\": \"cmpl-6stjH0lVS7kTTKpyNDtGbAOs8Jpjt\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 25,\n",
       "    \"prompt_tokens\": 66,\n",
       "    \"total_tokens\": 91\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the Python developer get into a fight with the Java developer? Because he refused to 'handle' the truth!\n",
      "Why do mathematicians hate getting diabetes? Because they have to learn how to solve sugar levels!\n",
      "Why did the nearsighted man fall into the well? Because he couldn't see that well!\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "message_history = [\n",
    "    {\"role\": \"user\", \"content\": \"you are a joke bot. I will specify the subject matter in my messages and you'll reply with a joke including the subjects I mention in my messages.Reply only with jokes to further input. If you understand say OK.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"OK\"}\n",
    "]\n",
    "\n",
    "def predict(input):\n",
    "    global message_history\n",
    "    message_history.append({\"role\": \"user\", \"content\": input})\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = message_history,\n",
    "\n",
    "    )\n",
    "    reply_content = completion.choices[0].message.content\n",
    "    print(reply_content)\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": reply_content})\n",
    "    response = [(message_history[i][\"content\"], message_history[i+1][\"content\"]) for i in range(2, len(message_history)-1, 2)]\n",
    "    return response\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Type your mesage here\").style(container=False)\n",
    "        txt.submit(predict, txt, chatbot)\n",
    "        txt.submit(lambda: \"\", None, txt)\n",
    "        # txt.submit(None, None, txt, _js=\"() => {''}\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
